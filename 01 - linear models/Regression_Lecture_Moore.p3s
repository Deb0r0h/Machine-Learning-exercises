/#!/usr/bin/p3i

/# #  Linear Regression on House Pricing Dataset
/# We consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.
/# 
/# Link to the dataset.
/# https://www.kaggle.com/harlfoxem/housesalesprediction
/# 
/# For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict.
/# 
/# A version of the dataset is in the ZIP file where you got this notebook.


/#put here your ``numero di matricola''
ID_number := #RANDOM VALUE


use std

random.seed(ID_number)


/# ## Import Data
/# Load the data from a .csv file

filename := "kc_house_data.csv"

/#load the data (df is the data frame)
df ::= file.csv.open(filename, r, sep = ',')?

/#let's print out the data
cmd << df <<


/# # A quick overview of data
/# 
/# Now let's clean the data and inspect it using the method describe().

/#remove the data samples with missing values (NaN)
df.dropnan() 

df.describe()

/# Extract input and output data. We want to predict the price by using features other than id as input.

/# # Split data in training and test set
/# 
/# Given $m$ total data, keep $m_t$ data as training data, and $m_{test}:=m - m_t$ for test data. For instance one can take $m_t=\frac{3}{4}m $ of the data as training, and $m_{test}=\frac{m}{4}$ as testing. Let us define
/# - $S_{t}$ the training data set
/# - $S_{test}$ the testing data set
/# 
/# 
/# The reason for this splitting is as follows:
/# 
/# TRAINING DATA: The training data are used to compute the empirical loss
/# $$
/# L_S(h) = \frac{1}{m_t} \sum_{z_i \in S_{t}} \ell(h,z_i)
/# $$
/# which is used to get $h_S$ in a given model class ${\cal H}$.
/# i.e. 
/# $$
/# h_S = {\rm arg\; min}_{h \in {\cal H}} \, L_S(h)
/# $$
/# 
/# TESTING DATA: Last, the test data set can be used to estimate the performance of the chosen hypothesis $h_{S}$ using:
/# 
/# $$
/# L_{\cal D}(h_S)  \simeq \frac{1}{m_{test}} \sum_{ z_i \in S_{test}} \ell(h_{S},z_i)
/# $$
/# 
/# **TO DO: split the data in training and test sets (suggestion: use $m_t=\left\lfloor\frac{3}{4}m\right\rfloor $, $m_{test} = m-m_t$)**

/#let's consider only the values in the DataFrame
Data ::= df.values /# observed values without header

/# m = number of input samples
m := Data.shape[0]

cmd << "Total number of samples: "  << m <<

/#size of training dataset
size_training := 3/4

cmd << "Number of samples in training data: " << (int) size_training*m <<

/#shuffle the data (to make sure we get a random split)

Data.shuffle()

/#divide data into matrix X of features and target vector Y 
/#Data: id, date, price, bedrooms, bathrooms, ...
Y := Data[:,2]
X := Data[:,3:]

/#training data

X_training := X[:size_training]
Y_training := Y[:size_training]
cmd << "Training input data size: " << X_training.shape <<
cmd << "Training output data size: " << Y_training.shape <<

/#test data, to be used to estimate the true loss of the final model
X_test := X[size_training:]
Y_test := Y[size_training:]
cmd << "Test input data size: " << X_test.shape <<
cmd << "Test output data size: " << Y_test.shape <<


/# # Data Normalization
/# 
/# It is common practice in Statistics and Machine Learning to scale the data (= each variable) so that it is centered (zero mean) and has standard deviation equal to $1$. This helps in terms of numerical conditioning of the (inverse) problems of learning the model (the coefficients of the linear regression in this case), as well as to give the same scale to all the coefficients. 

/# scale the data: standardize the training feature matrix
/# create scaler
scaler := data.StandardScaler.fit(X_training)

/# apply scaler
X_training_scaled := scaler.transform(X_training)
cmd << "Mean of the training input data:" <<< X_training_scaled.mean(axis=0) <<
cmd << "Std of the training input data:" <<< X_training_scaled.std(axis=0) <<

/# now we scale the test feature matrix using the same transformation used
/# for the training dataset, since the weights of the model will be learned
/# data scaled according to such transformation

X_test_scaled := scaler.transform(X_test)
cmd << "Mean of the test input data:" <<< X_test_scaled.mean(axis=0) <<
cmd << "Std of the test input data:" <<< X_test_scaled.std(axis=0) <<


/# # Model Training 
/# 
/# The model is trained minimizing the empirical error
/# $$
/# L_S(h) := \frac{1}{N_t} \sum_{z_i \in S_{t}} \ell(h,z_i)
/# $$
/# When the loss function is the quadratic loss
/# $$
/# \ell(h,z) := (y - h(x))^2
/# $$
/# we define  the Residual Sum of Squares (RSS) as
/# $$
/# RSS(h):= \sum_{z_i \in S_{t}} \ell(h,z_i) = \sum_{z_i \in S_{t}} (y_i - h(x_i))^2
/# $$ so that the training error becomes
/# $$
/# L_S(h) = \frac{RSS(h)}{m_t}
/# $$
/# 
/# We recal that, for linear models we have $h(x) = <w,x>$ and the Empirical error $L_S(h)$ can be written
/# in terms of the vector of parameters $w$ in the form
/# $$
/# L_S(w) = \frac{1}{m_t} \|Y - X w\|^2
/# $$
/# where $Y$ and $X$ are the matrices whose $i-$th row are, respectively, the output data $y_i$ and the input vectors $x_i^\top$.
/# 
/# The least squares solution is given by the expression
/# $$
/# \hat w = {\rm arg\;min}_w L_S(w) = (X^\top X)^{-1} X^\top Y
/# $$
/# When the matrix $X$ is not invertible (or even when it is invertible), the solution can be computed using the Moore-Penrose pseudonverse $(X^\top X)^{\dagger}$ of $(X^\top X)$
/# $$
/# \hat w = (X^\top X)^{\dagger} X^\top Y
/# $$
/# The Moore-Penrose pseudoinverse $A^\dagger$ of a matrix $A \in \mathbb{R}^{m\times n}$ can be expressed in terms of the Singular Value Decomposition (SVD) as follows:
/# let $A\in \mathbb{R}^{m\times n}$ be of rank $r\leq {\rm min}(n,m)$ and let  
/# $$
/#  A = USV^\top
/#  $$
/#  be the singular value decomposition of  $A$ where  
/#  $$
/#  S = {\rm diag}\{s_1,s_2,..,s_r\}
/#  $$
/#  Then 
/#  $$
/#  A^\dagger =V S^{-1} U^\top 
/#  $$
/#  
/#  In practice some of the singular values may be very small (e.g. $<1e-10$). Therefore it makes sense to  first approximate the matrix $A$ truncating the SVD and then using the pseudoinverse formula.
/#  
/#  More specifically, let us postulate that, given a threshold $T_h$ (e.g $=1e-12$), we have $\sigma_i<T_h$, for $i=\hat r + 1,..,r$. Then we can approximate (by SVD truncation) $A$ using:
/#  
/#  $$A = USV^\top =U \,{\rm diag}\{s_1,s_2,..,s_r\}\, V^\top \simeq \hat A_r = U\,{\rm diag}\{s_1,s_2,..,s_{\hat r}, 0,..,0\}\,V^\top
/#  $$
/#  So that 
/#  $$
/#  A^\dagger \simeq \hat A_r^\dagger:= V \,{\rm diag}\{1/s_1,1/s_2,..,1/s_{\hat r}, 0,..,0\}\, U^\top
/#  $$
/#   
/#  In numpy, the Moore-Penrose pseudo-inverse of a matrix can be computed using the method numpy.linalg.pinv(...), which takes among its parameters the threshold for truncating the singular values to 0.
/#   
/#  **TO DO: compute the linear regression coefficients according to the description above (using numpy.linalg.pinv(...) )**


/#compute linear regression coefficients for training data

/#number of samples in the training set
m_training := X_training_scaled.shape[0]

/#number of samples in the test set
m_test := X_test_scaled.shape[0]

/# add a 1 at the beginning of each sample for training, and testing
/# the numpy function hstack is useful for such operation
X_training' := arr.ones((m_training,1)) # X_training_scaled

X_test' := arr.ones((m_test,1)) # X_test_scaled

/# set precision under which singular values are considered as zeros
prec := 1e-10  

/# compute Moore-Penrose pseudoinverse of the matrix you need to compute 
/# the weights of the model
A_inv = linalg.pinv(X_training'.T @ X_training', prec)

/# now compute the weights and print them
w_hand = A_inv @ X_training'.T @ Y_training

cmd << "LS coefficients by hand: " << w_hand <<

/# compute Residual Sums of Squares by hand
RSStr_hand := linalg.sqnorm(Y_training- X_training' @ w_hand)

/# print the RSS
cmd << "RSS by hand: " <<  RSStr_hand <<

/# print the empirical risk
cmd << "Empirical risk by hand: " << RSStr_hand/m_training <<


/# ## Data prediction 
/# 
/# Compute the output predictions on both training and test set and compute the Residual Sum of Squares (RSS) defined above, the Empirical Loss and the quantity $R^2$ where
/# $$
/# R^2 = 1 - \frac{\sum_{z_i \in S_t} (y_i - \hat y_i )^2}{\sum_{z_i \in S_t} (y_i - \bar y)^2}  \quad \quad \bar y = \frac{1}{m_t} \sum_{z_i \in S_t} y_i
/# $$
/# is the so-called "Coefficient of determination" (COD).
/# 
/# **TO DO Compute these quantities on  training and test data.**
/# 

/#compute predictions on training and test
prediction_training := X_training' @ w_hand
prediction_test := X_test' @ w_hand

/#what about the RSS and empirical loss for points in the test data?
RSS_test := linalg.sqnorm(Y_test - X_test' @ w_hand)

/#print("RSS on test data:",  RSS_test)
cmd << "Generalization error estimated on test data (i.e., empirical loss on test data):" << RSS_test/m_test <<

/#another measure of how good our linear fit is given by the following (that is R^2)
measure_training := 1 - linalg.sqnorm(Y_training - prediction_training) / linalg.sqnorm(Y_training - Y_training.mean())
measure_test := 1 - linalg.sqnorm(Y_test - prediction_test) / linalg.sqnorm(Y_test - Y_test.mean())

cmd << "Measure on Training Data (R^2):" <<< measure_training <<
cmd << "Measure on Test Data(R^2): " << measure_test <<
